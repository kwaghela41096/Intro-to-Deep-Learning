{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1b1a6c7c8b974d8cb39a7899354a4808": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ea84cc4ebea64f46a58c3f4225ec0b99",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e5544a9279234a0da4595548f9acd8a5",
              "IPY_MODEL_ba285639c1b646ff937e17c5b80b9612"
            ]
          }
        },
        "ea84cc4ebea64f46a58c3f4225ec0b99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e5544a9279234a0da4595548f9acd8a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_aa9f6256ecf7473897d28298aabb6cc5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_98c7b91e807f47cdb5ea6cde041dc8fd"
          }
        },
        "ba285639c1b646ff937e17c5b80b9612": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_85d7e609f7bf417aa698bae721a3f107",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a058d1e16ab94af9aad4f092867de4a3"
          }
        },
        "aa9f6256ecf7473897d28298aabb6cc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "98c7b91e807f47cdb5ea6cde041dc8fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "85d7e609f7bf417aa698bae721a3f107": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a058d1e16ab94af9aad4f092867de4a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installs"
      ],
      "metadata": {
        "id": "UR4qfYrVoO4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructions to run the code"
      ],
      "metadata": {
        "id": "Zbfxig59unzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running this notebook would ensure that the model runs on the data provided, The data for this homework comes from 11785 course at Carnegie Mellon university. The specifics are written before each and every main block:"
      ],
      "metadata": {
        "id": "Qfv2QY9Qusbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code follows the following structure:\n",
        "* Importing the required packages\n",
        "* Defining the Dataloaders to use it for training and testing\n",
        "* Checking the dataloaders\n",
        "* Defining the model\n",
        "* Training and evaluation loop\n",
        "* Training\n",
        "* Generating submissions\n"
      ],
      "metadata": {
        "id": "iCFDd6YJvjHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wandb\n",
        "\n",
        "You will need to fetch your api key from wandb.ai"
      ],
      "metadata": {
        "id": "rd5aNaLVoR_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vGCaaKVndQp",
        "outputId": "07e760af-9210-4bcb-97d3-eb8e630f22a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Nov 22 04:00:12 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -q"
      ],
      "metadata": {
        "id": "mA9qZoIDcx-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4872a51-d979-4dbc-b9ad-530dce4b5641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.9 MB 5.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 168 kB 83.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 84.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 86.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 92.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 95.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 83.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 158 kB 85.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 95.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 88.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 27.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 89.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 93.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 97.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 88.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 156 kB 94.0 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login(key=\"e88e5782628fb1039c9df3e4f75ae46a961a0979\")"
      ],
      "metadata": {
        "id": "PiDduMaDIARE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba4f3af5-b86f-408b-a0c0-55dc7673016e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Levenshtein\n",
        "\n",
        "This may take a while"
      ],
      "metadata": {
        "id": "ONgAWhqdoYy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-Levenshtein\n",
        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "!pip install wget\n",
        "%cd ctcdecode\n",
        "!pip install .\n",
        "%cd ..\n",
        "\n",
        "!pip install torchsummaryX"
      ],
      "metadata": {
        "id": "SS7a7xeEoaV9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b32a461-e775-45ba-a9cc-b29afc0c2b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.20.8-py3-none-any.whl (9.4 kB)\n",
            "Collecting Levenshtein==0.20.8\n",
            "  Downloading Levenshtein-0.20.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
            "  Downloading rapidfuzz-2.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 74.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.20.8 python-Levenshtein-0.20.8 rapidfuzz-2.13.2\n",
            "Cloning into 'ctcdecode'...\n",
            "remote: Enumerating objects: 1102, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 1102 (delta 16), reused 32 (delta 14), pack-reused 1063\u001b[K\n",
            "Receiving objects: 100% (1102/1102), 782.27 KiB | 12.42 MiB/s, done.\n",
            "Resolving deltas: 100% (529/529), done.\n",
            "Submodule 'third_party/ThreadPool' (https://github.com/progschj/ThreadPool.git) registered for path 'third_party/ThreadPool'\n",
            "Submodule 'third_party/kenlm' (https://github.com/kpu/kenlm.git) registered for path 'third_party/kenlm'\n",
            "Cloning into '/content/ctcdecode/third_party/ThreadPool'...\n",
            "remote: Enumerating objects: 82, done.        \n",
            "remote: Total 82 (delta 0), reused 0 (delta 0), pack-reused 82        \n",
            "Cloning into '/content/ctcdecode/third_party/kenlm'...\n",
            "remote: Enumerating objects: 14102, done.        \n",
            "remote: Counting objects: 100% (415/415), done.        \n",
            "remote: Compressing objects: 100% (289/289), done.        \n",
            "remote: Total 14102 (delta 127), reused 383 (delta 112), pack-reused 13687        \n",
            "Receiving objects: 100% (14102/14102), 5.89 MiB | 22.77 MiB/s, done.\n",
            "Resolving deltas: 100% (8007/8007), done.\n",
            "Submodule path 'third_party/ThreadPool': checked out '9a42ec1329f259a5f4881a291db1dcb8f2ad9040'\n",
            "Submodule path 'third_party/kenlm': checked out '35835f1ac4884126458ac89f9bf6dd9ccad561e0'\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=ef631a21a843e68cfd20c29a128094a0225772a74f3f320c41b4364a0ddefb97\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "/content/ctcdecode\n",
            "Processing /content/ctcdecode\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Building wheels for collected packages: ctcdecode\n",
            "  Building wheel for ctcdecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ctcdecode: filename=ctcdecode-1.0.3-cp37-cp37m-linux_x86_64.whl size=13258528 sha256=46231b614f5168e36b9a1fb80014d144db36e6c9b9ec40448d66b987e8404b24\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v3z3_wa4/wheels/da/bb/b4/233de9fd7927245208e27bcf688bf5680ae3f3874be2895eef\n",
            "Successfully built ctcdecode\n",
            "Installing collected packages: ctcdecode\n",
            "Successfully installed ctcdecode-1.0.3\n",
            "/content\n",
            "Collecting torchsummaryX\n",
            "  Downloading torchsummaryX-1.3.0-py3-none-any.whl (3.6 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.21.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsummaryX) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsummaryX) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torchsummaryX) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchsummaryX) (3.10.0.2)\n",
            "Installing collected packages: torchsummaryX\n",
            "Successfully installed torchsummaryX-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## imports"
      ],
      "metadata": {
        "id": "IWVONJxCobPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torchaudio.transforms as tat\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# imports for decoding and distance calculation\n",
        "import ctcdecode\n",
        "import Levenshtein\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ZTCIXoof2f",
        "outputId": "75d866f1-2785-4a5b-bbea-1dbc91594a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle Setup"
      ],
      "metadata": {
        "id": "gg3-yJ8tok34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"kwaghela04\",\"key\":\"02aa393f704d522832a2ea55ec85f87a\"}')\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "AdUelfGhom1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a8447b8-ff1d-44fe-ec7b-959da535a1be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaggle==1.5.8\n",
            "  Using cached kaggle-1.5.8-py3-none-any.whl\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.8\n",
            "    Uninstalling kaggle-1.5.8:\n",
            "      Successfully uninstalled kaggle-1.5.8\n",
            "Successfully installed kaggle-1.5.8\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c 11-785-f22-hw3p2"
      ],
      "metadata": {
        "id": "dSjBwfXeoq4B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6405710d-9637-4698-d699-8133f77fd43d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 11-785-f22-hw3p2.zip to /content\n",
            "100% 8.87G/8.88G [00:51<00:00, 257MB/s]\n",
            "100% 8.88G/8.88G [00:51<00:00, 184MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This will take a couple minutes, but you should see at least the following:\n",
        "11-785-f22-hw3p2.zip  ctcdecode  hw3p2\n",
        "'''\n",
        "!unzip -q 11-785-f22-hw3p2.zip\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a97cb2d8-007a-418b-acd4-f9afcf5b06df",
        "id": "_ruxWP60LCQA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11-785-f22-hw3p2.zip  ctcdecode  hw3p2\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Drive"
      ],
      "metadata": {
        "id": "R9v5ewZDMpYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Cp-716IMZRd",
        "outputId": "9f91b4ce-9e3b-4430-8e7f-a20eb736f9c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and Dataloader"
      ],
      "metadata": {
        "id": "2ORNHnSFroP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ARPABET PHONEME MAPPING\n",
        "# DO NOT CHANGE\n",
        "# This overwrites the phonetics.py file.\n",
        "\n",
        "CMUdict_ARPAbet = {\n",
        "    \"\" : \" \", # BLANK TOKEN\n",
        "    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\", \n",
        "    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\", \n",
        "    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\", \n",
        "    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\", \n",
        "    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\", \n",
        "    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\", \n",
        "    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\", \n",
        "    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
        "    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"}\n",
        "\n",
        "CMUdict = list(CMUdict_ARPAbet.keys())\n",
        "ARPAbet = list(CMUdict_ARPAbet.values())\n",
        "\n",
        "\n",
        "PHONEMES = CMUdict\n",
        "mapping = CMUdict_ARPAbet\n",
        "LABELS = ARPAbet"
      ],
      "metadata": {
        "id": "k0v7wHRWrqH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You might want to play around with the mapping as a sanity check here"
      ],
      "metadata": {
        "id": "eN2kcxwXLLBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Data"
      ],
      "metadata": {
        "id": "agmNBKf4JrLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, partition='train-clean-360', context=0, offset=0): \n",
        "        self.mfcc_dir = 'hw3p2/'+partition+'/mfcc'\n",
        "        self.transcript_dir = 'hw3p2/'+partition+'/transcript/raw'\n",
        "\n",
        "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
        "        self.transcript_files = sorted(os.listdir(self.transcript_dir))\n",
        "\n",
        "        self.PHONEMES = PHONEMES\n",
        "\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "\n",
        "        for i in range(0, len(self.mfcc_files)):\n",
        "            mfcc = np.load(self.mfcc_dir+\"/\"+self.mfcc_files[i])\n",
        "            mfcc = (mfcc - np.mean(mfcc, axis=0))/np.std(mfcc, axis=0)\n",
        "            transcript = np.delete(np.load(self.transcript_dir+\"/\"+self.transcript_files[i]), (0, -1)) \n",
        "            self.mfccs.append(mfcc)\n",
        "            self.transcripts.append(transcript)\n",
        "\n",
        "        self.length = len(self.mfccs)\n",
        "        t = []\n",
        "        for tr in self.transcripts:\n",
        "          t.append(np.array([self.PHONEMES.index(i) for i in tr]))\n",
        "        self.transcripts = t\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        \n",
        "        frames = self.mfccs[ind] \n",
        "        frames = torch.FloatTensor(frames) # Convert to tensors\n",
        "        phoneme = torch.tensor(self.transcripts[ind])       \n",
        "\n",
        "        return frames, phoneme\n",
        "\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "        batch_mfcc = [b[0] for b in batch] # TODO\n",
        "        batch_transcript = [b[1] for b in batch] # TODO\n",
        "        batch_mfcc_pad = torch.nn.utils.rnn.pad_sequence(batch_mfcc, batch_first=True, padding_value=0.0) # TODO\n",
        "        lengths_mfcc = [len(m) for m in batch_mfcc] # TODO \n",
        "        batch_transcript_pad = torch.nn.utils.rnn.pad_sequence(batch_transcript, batch_first=True, padding_value=0.0) # TODO\n",
        "        lengths_transcript = [len(t) for t in batch_transcript] # TODO\n",
        "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n",
        "\n",
        "       "
      ],
      "metadata": {
        "id": "afd0_vlbJmr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Data"
      ],
      "metadata": {
        "id": "hqDrxeHfJw4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Dataloader\n",
        "#TODO\n",
        "class AudioDatasetTest(torch.utils.data.Dataset):\n",
        "   def __init__(self, partition='test-clean'): \n",
        "\n",
        "        self.mfcc_dir = 'hw3p2/'+partition+'/mfcc'\n",
        "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
        "        self.PHONEMES = PHONEMES\n",
        "        self.length = len(self.mfcc_files)\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "        for i in range(0, len(self.mfcc_files)):\n",
        "            mfcc = np.load(self.mfcc_dir+\"/\"+self.mfcc_files[i])\n",
        "            mfcc = (mfcc - np.mean(mfcc, axis=0))/np.std(mfcc, axis=0)\n",
        "            self.mfccs.append(mfcc)\n",
        "\n",
        "        self.length = len(self.mfccs)\n",
        "\n",
        "   def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "   def __getitem__(self, ind):\n",
        "        \n",
        "        frames = self.mfccs[ind] \n",
        "        frames = torch.FloatTensor(frames) # Convert to tensors     \n",
        "        return frames\n",
        "\n",
        "   def collate_fn(self,batch):\n",
        "        batch_mfcc = [b for b in batch] # TODO\n",
        "        batch_mfcc_pad = torch.nn.utils.rnn.pad_sequence(batch_mfcc, batch_first=True, padding_value=0.0) # TODO\n",
        "        lengths_mfcc = [len(m) for m in batch_mfcc] # TODO \n",
        "        return batch_mfcc_pad, torch.tensor(lengths_mfcc)"
      ],
      "metadata": {
        "id": "HrLS1wfVJppA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data - Hyperparameters"
      ],
      "metadata": {
        "id": "Pt-veYcdL6Fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64 # Increase if your device can handle it\n",
        "\n",
        "transforms = [] # set of tranformations"
      ],
      "metadata": {
        "id": "4icymeX1ImUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data loaders"
      ],
      "metadata": {
        "id": "NmuPk9J6L8dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gc \n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_kG0gU2x4hH",
        "outputId": "2204fe63-5be6-4f17-ec87-1298fa714cb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "264"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create objects for the dataset class\n",
        "train_data = AudioDataset() #TODO\n",
        "val_data = AudioDataset(partition='dev-clean') \n",
        "test_data = AudioDatasetTest() #TODO\n",
        "\n",
        "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train_data, num_workers= 4,\n",
        "                                           batch_size=BATCH_SIZE, pin_memory= True,\n",
        "                                           shuffle= True, collate_fn=train_data.collate_fn)\n",
        "val_loader = torch.utils.data.DataLoader(val_data, num_workers= 2,\n",
        "                                         batch_size=BATCH_SIZE, pin_memory= True,\n",
        "                                         shuffle= False, collate_fn=val_data.collate_fn)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, num_workers= 2, \n",
        "                                          batch_size=BATCH_SIZE, pin_memory= True, \n",
        "                                          shuffle= False, collate_fn=test_data.collate_fn)\n",
        "\n",
        "print(\"Batch size: \", BATCH_SIZE)\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ],
      "metadata": {
        "id": "4mzoYfTKu14s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9cd4dc6-13e9-430b-9e48-b7e225b866fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size:  64\n",
            "Train dataset samples = 104014, batches = 1626\n",
            "Val dataset samples = 2703, batches = 43\n",
            "Test dataset samples = 2620, batches = 41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check\n",
        "for data in train_loader:\n",
        "    x, y, lx, ly = data\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break "
      ],
      "metadata": {
        "id": "cXMtwyviKaxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5834c59-105f-421f-89fe-5e206269aa65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1651, 15]) torch.Size([64, 186]) torch.Size([64]) torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for data in val_loader:\n",
        "    x, y, lx, ly = data\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break "
      ],
      "metadata": {
        "id": "I3NvbAwcrg_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "578d7d60-64eb-400d-8656-405d4314c29d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1907, 15]) torch.Size([64, 217]) torch.Size([64]) torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for data in test_loader:\n",
        "    x, lx = data\n",
        "    print(x.shape, lx.shape)\n",
        "    break "
      ],
      "metadata": {
        "id": "SAtzQF-1rjrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "952619a1-7b6f-4005-e35f-c78eb810ebac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 825, 15]) torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Config"
      ],
      "metadata": {
        "id": "Ly4mjUUUuJhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUT_SIZE = len(LABELS)\n",
        "OUT_SIZE"
      ],
      "metadata": {
        "id": "RZ-qQ_Sf-LIu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e986ac77-355a-47c7-d38c-b76130883244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Model"
      ],
      "metadata": {
        "id": "HLad4pChcuvX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class is defined for locked dropout regularization"
      ],
      "metadata": {
        "id": "RFnI94m1zY4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "import sys\n",
        "class LockedDropout(nn.Module):\n",
        "    def forward(self, x, dropout=0.2):\n",
        "        x, lx = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "        m = x.data.new(x.size(0), 1, x.size(2)).bernoulli_(1 - dropout)\n",
        "        mask = Variable(m, requires_grad=False) / (1 - dropout)\n",
        "        mask = mask.expand_as(x)\n",
        "        out = mask * x\n",
        "        return torch.nn.utils.rnn.pack_padded_sequence(out, lengths=lx, batch_first=True, enforce_sorted=False)"
      ],
      "metadata": {
        "id": "yrdkpqgGzU2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A final model with a CNN embedding layer followed by a bidirectional block and finally a linear classification unit for final classifications"
      ],
      "metadata": {
        "id": "DH4qFL8Uze_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "        self.embedding = nn.Sequential(\n",
        "              nn.Conv1d(15, 256, 1),\n",
        "              nn.BatchNorm1d(256),\n",
        "              nn.GELU(),\n",
        "\n",
        "              nn.Conv1d(256, 512, 1),\n",
        "              nn.BatchNorm1d(512),\n",
        "              nn.GELU(),\n",
        "\n",
        "              nn.Conv1d(512, 512, 1),\n",
        "              nn.BatchNorm1d(512),\n",
        "              nn.GELU(),\n",
        "        )\n",
        "        self.lstms = nn.Sequential(\n",
        "            nn.LSTM(input_size = 512, hidden_size = 256, num_layers = 4, bidirectional=True, dropout=0.2, batch_first=True),\n",
        "        )\n",
        "        self.classification = nn.Sequential(\n",
        "            nn.Linear(512, 256),nn.Dropout(0.2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(256, OUT_SIZE)\n",
        "        )\n",
        "        self.logSoftmax = nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, x, lx):\n",
        "        x = torch.permute(x, (0,2,1))\n",
        "        x = torch.permute(self.embedding(x), (0,2,1))\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(x, lengths=lx, batch_first=True, enforce_sorted=False)\n",
        "        out,_ = self.lstms(packed)\n",
        "        unpacked,len_unpacked = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
        "        out = self.classification(unpacked)\n",
        "        out = self.logSoftmax(out)\n",
        "        return out, len_unpacked"
      ],
      "metadata": {
        "id": "nh53-udSj2LC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INIT"
      ],
      "metadata": {
        "id": "tUThsowyQdN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = Network().to(device)\n",
        "summary(model, x.to(device), lx) # x and lx come from the sanity check above :)"
      ],
      "metadata": {
        "id": "CGoiXd70tb5z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e7464de5-69f9-4305-ba9a-c8a8380dbed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================================================\n",
            "                              Kernel Shape    Output Shape    Params  \\\n",
            "Layer                                                                  \n",
            "0_embedding.Conv1d_0          [15, 256, 1]  [64, 256, 825]    4.096k   \n",
            "1_embedding.BatchNorm1d_1            [256]  [64, 256, 825]     512.0   \n",
            "2_embedding.GELU_2                       -  [64, 256, 825]         -   \n",
            "3_embedding.Conv1d_3         [256, 512, 1]  [64, 512, 825]  131.584k   \n",
            "4_embedding.BatchNorm1d_4            [512]  [64, 512, 825]    1.024k   \n",
            "5_embedding.GELU_5                       -  [64, 512, 825]         -   \n",
            "6_embedding.Conv1d_6         [512, 512, 1]  [64, 512, 825]  262.656k   \n",
            "7_embedding.BatchNorm1d_7            [512]  [64, 512, 825]    1.024k   \n",
            "8_embedding.GELU_8                       -  [64, 512, 825]         -   \n",
            "9_lstms.LSTM_0                           -    [28434, 512]  6.30784M   \n",
            "10_classification.Linear_0      [512, 256]  [64, 825, 256]  131.328k   \n",
            "11_classification.Dropout_1              -  [64, 825, 256]         -   \n",
            "12_classification.GELU_2                 -  [64, 825, 256]         -   \n",
            "13_classification.Linear_3      [256, 256]  [64, 825, 256]   65.792k   \n",
            "14_classification.GELU_4                 -  [64, 825, 256]         -   \n",
            "15_classification.Linear_5       [256, 43]   [64, 825, 43]   11.051k   \n",
            "16_logSoftmax                            -   [64, 825, 43]         -   \n",
            "\n",
            "                             Mult-Adds  \n",
            "Layer                                   \n",
            "0_embedding.Conv1d_0            3.168M  \n",
            "1_embedding.BatchNorm1d_1        256.0  \n",
            "2_embedding.GELU_2                   -  \n",
            "3_embedding.Conv1d_3         108.1344M  \n",
            "4_embedding.BatchNorm1d_4        512.0  \n",
            "5_embedding.GELU_5                   -  \n",
            "6_embedding.Conv1d_6         216.2688M  \n",
            "7_embedding.BatchNorm1d_7        512.0  \n",
            "8_embedding.GELU_8                   -  \n",
            "9_lstms.LSTM_0               6.291456M  \n",
            "10_classification.Linear_0    131.072k  \n",
            "11_classification.Dropout_1          -  \n",
            "12_classification.GELU_2             -  \n",
            "13_classification.Linear_3     65.536k  \n",
            "14_classification.GELU_4             -  \n",
            "15_classification.Linear_5     11.008k  \n",
            "16_logSoftmax                        -  \n",
            "----------------------------------------------------------------------------------\n",
            "                           Totals\n",
            "Total params            6.916907M\n",
            "Trainable params        6.916907M\n",
            "Non-trainable params          0.0\n",
            "Mult-Adds             334.071552M\n",
            "==================================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9a9806cf-46ea-4b76-ba90-375a63a067df\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_embedding.Conv1d_0</th>\n",
              "      <td>[15, 256, 1]</td>\n",
              "      <td>[64, 256, 825]</td>\n",
              "      <td>4096.0</td>\n",
              "      <td>3168000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_embedding.BatchNorm1d_1</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[64, 256, 825]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_embedding.GELU_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 256, 825]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_embedding.Conv1d_3</th>\n",
              "      <td>[256, 512, 1]</td>\n",
              "      <td>[64, 512, 825]</td>\n",
              "      <td>131584.0</td>\n",
              "      <td>108134400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_embedding.BatchNorm1d_4</th>\n",
              "      <td>[512]</td>\n",
              "      <td>[64, 512, 825]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_embedding.GELU_5</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 512, 825]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6_embedding.Conv1d_6</th>\n",
              "      <td>[512, 512, 1]</td>\n",
              "      <td>[64, 512, 825]</td>\n",
              "      <td>262656.0</td>\n",
              "      <td>216268800.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_embedding.BatchNorm1d_7</th>\n",
              "      <td>[512]</td>\n",
              "      <td>[64, 512, 825]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8_embedding.GELU_8</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 512, 825]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_lstms.LSTM_0</th>\n",
              "      <td>-</td>\n",
              "      <td>[28434, 512]</td>\n",
              "      <td>6307840.0</td>\n",
              "      <td>6291456.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_classification.Linear_0</th>\n",
              "      <td>[512, 256]</td>\n",
              "      <td>[64, 825, 256]</td>\n",
              "      <td>131328.0</td>\n",
              "      <td>131072.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11_classification.Dropout_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 825, 256]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12_classification.GELU_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 825, 256]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13_classification.Linear_3</th>\n",
              "      <td>[256, 256]</td>\n",
              "      <td>[64, 825, 256]</td>\n",
              "      <td>65792.0</td>\n",
              "      <td>65536.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14_classification.GELU_4</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 825, 256]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15_classification.Linear_5</th>\n",
              "      <td>[256, 43]</td>\n",
              "      <td>[64, 825, 43]</td>\n",
              "      <td>11051.0</td>\n",
              "      <td>11008.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16_logSoftmax</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 825, 43]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9a9806cf-46ea-4b76-ba90-375a63a067df')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9a9806cf-46ea-4b76-ba90-375a63a067df button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9a9806cf-46ea-4b76-ba90-375a63a067df');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                              Kernel Shape  ...    Mult-Adds\n",
              "Layer                                       ...             \n",
              "0_embedding.Conv1d_0          [15, 256, 1]  ...    3168000.0\n",
              "1_embedding.BatchNorm1d_1            [256]  ...        256.0\n",
              "2_embedding.GELU_2                       -  ...          NaN\n",
              "3_embedding.Conv1d_3         [256, 512, 1]  ...  108134400.0\n",
              "4_embedding.BatchNorm1d_4            [512]  ...        512.0\n",
              "5_embedding.GELU_5                       -  ...          NaN\n",
              "6_embedding.Conv1d_6         [512, 512, 1]  ...  216268800.0\n",
              "7_embedding.BatchNorm1d_7            [512]  ...        512.0\n",
              "8_embedding.GELU_8                       -  ...          NaN\n",
              "9_lstms.LSTM_0                           -  ...    6291456.0\n",
              "10_classification.Linear_0      [512, 256]  ...     131072.0\n",
              "11_classification.Dropout_1              -  ...          NaN\n",
              "12_classification.GELU_2                 -  ...          NaN\n",
              "13_classification.Linear_3      [256, 256]  ...      65536.0\n",
              "14_classification.GELU_4                 -  ...          NaN\n",
              "15_classification.Linear_5       [256, 43]  ...      11008.0\n",
              "16_logSoftmax                            -  ...          NaN\n",
              "\n",
              "[17 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Config"
      ],
      "metadata": {
        "id": "IBwunYpyugFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_config = {\n",
        "    \"beam_width\" : 4,\n",
        "    \"lr\" : 2e-3,\n",
        "    \"epochs\" : 80\n",
        "    } # Feel free to add more items here"
      ],
      "metadata": {
        "id": "MN82c3KpLup8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CTCLoss()\n",
        "lr = 1e-3\n",
        "optimizer = torch.optim.AdamW(params = model.parameters(), lr = lr, weight_decay = 2e-3)\n",
        "decoder = CTCBeamDecoder(labels = PHONEMES, beam_width = 4, log_probs_input = True)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer, mode='min', factor = 0.5, patience = 4, threshold = 0.01)"
      ],
      "metadata": {
        "id": "iGoozH2nd6KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Levenshtein"
      ],
      "metadata": {
        "id": "Jmc6_4eWL2Xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_levenshtein(h, y, lh, ly, decoder, PHONEME_MAP):\n",
        "    batch_size = h.shape[0]\n",
        "    dist = 0\n",
        "    beam_results, beam_scores, timesteps, out_lens = decoder.decode(h, seq_lens = lh)\n",
        "    for i in range(batch_size): \n",
        "        beam = beam_results[i][0][:out_lens[i][0]]\n",
        "\n",
        "        h_string = \"\".join([PHONEME_MAP[x] for x in beam])\n",
        "\n",
        "        y_sliced = y[i,0:ly[i]]\n",
        "        y_string = \"\".join([PHONEME_MAP[x] for x in y_sliced])\n",
        "        \n",
        "        dist += Levenshtein.distance(h_string, y_string)\n",
        "\n",
        "    dist /= batch_size\n",
        "\n",
        "    return dist"
      ],
      "metadata": {
        "id": "KHjnCDddL36E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = wandb.init(\n",
        "    name = \"hw3-med\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw3p2-ablations\", ### Project should be created in your wandb account \n",
        "    config = train_config ### Wandb Config for your run\n",
        ")"
      ],
      "metadata": {
        "id": "_ResP82cr7nh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239,
          "referenced_widgets": [
            "1b1a6c7c8b974d8cb39a7899354a4808",
            "ea84cc4ebea64f46a58c3f4225ec0b99",
            "e5544a9279234a0da4595548f9acd8a5",
            "ba285639c1b646ff937e17c5b80b9612",
            "aa9f6256ecf7473897d28298aabb6cc5",
            "98c7b91e807f47cdb5ea6cde041dc8fd",
            "85d7e609f7bf417aa698bae721a3f107",
            "a058d1e16ab94af9aad4f092867de4a3"
          ]
        },
        "outputId": "13e959d3-2dc1-409e-f242-b006824f96f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:2bwhttaw) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b1a6c7c8b974d8cb39a7899354a4808",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">hw3-med</strong>: <a href=\"https://wandb.ai/kwaghela04/hw3p2-ablations/runs/2bwhttaw\" target=\"_blank\">https://wandb.ai/kwaghela04/hw3p2-ablations/runs/2bwhttaw</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20221122_040949-2bwhttaw/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Successfully finished last run (ID:2bwhttaw). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221122_042014-1l19qb3z</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/kwaghela04/hw3p2-ablations/runs/1l19qb3z\" target=\"_blank\">hw3-med</a></strong> to <a href=\"https://wandb.ai/kwaghela04/hw3p2-ablations\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "6fLLj5KIMMOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eval function\n",
        "Writing a function to do one round of evaluations will help make your code more modular, you can, however, choose to skip this if you'd like it."
      ],
      "metadata": {
        "id": "kH0RAbCaMl9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def train(model, train_loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "    for i, data in (enumerate(train_loader)):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs, opLength = model(x, lx)\n",
        "            outputs = torch.permute(outputs, (1,0,2))\n",
        "            loss = criterion(outputs, y, opLength, ly)\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))))\n",
        "        scaler.scale(loss).backward() \n",
        "        scaler.step(optimizer) \n",
        "        scaler.update()\n",
        "        bar.update()\n",
        "\n",
        "    total_loss /= len(train_loader)\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "def validate(model, val_loader, optimizer, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    distance = 0\n",
        "    bar = tqdm(total=len(val_loader), dynamic_ncols=True, leave=False, position=0, desc='Validation')\n",
        "    for i, data in (enumerate(val_loader)):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs, opLength = model(x, lx)\n",
        "            outputs = torch.permute(outputs, (1,0,2))\n",
        "            loss = criterion(outputs, y, opLength, ly)\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        outputs = torch.permute(outputs, (1,0,2))\n",
        "        distance += calculate_levenshtein(outputs, y, opLength, ly, decoder, LABELS) \n",
        "        bar.update()\n",
        "\n",
        "    total_loss /= len(val_loader)\n",
        "    distance /= len(val_loader)\n",
        "    return total_loss, distance"
      ],
      "metadata": {
        "id": "2126OD5l0udt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,61):\n",
        "    print('Epoch', i)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    train_loss = train(model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_distance = validate(model, val_loader, optimizer, criterion)\n",
        "    \n",
        "    scheduler.step(val_distance)\n",
        "    print(scheduler._last_lr)\n",
        "\n",
        "    if i > 20 and i % 5 == 0:\n",
        "        checkpoint = { \n",
        "            'epoch': i,\n",
        "            'model': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'lr_sched': scheduler\n",
        "        }\n",
        "        prev_loss = val_loss\n",
        "        # torch.save(checkpoint, '/content/drive/checkpoint.pth')\n",
        "\n",
        "    print('Training Loss', train_loss)\n",
        "    print('Validation Loss', val_loss)\n",
        "    print('Validation distance', val_distance)"
      ],
      "metadata": {
        "id": "JR43E28rM9Ak",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67e07f3f-7293-4d8f-f81b-7a5ccb264b3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0005]\n",
            "Training Loss 0.16521841905287124\n",
            "Validation Loss 0.2596424928931303\n",
            "Validation distance 4.804408914728683\n",
            "Epoch 2\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0005]\n",
            "Training Loss 0.16081389641849758\n",
            "Validation Loss 0.2573622667858767\n",
            "Validation distance 4.744791666666666\n",
            "Epoch 3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0005]\n",
            "Training Loss 0.1570687897292597\n",
            "Validation Loss 0.24851747342320377\n",
            "Validation distance 4.580813953488372\n",
            "Epoch 4\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0005]\n",
            "Training Loss 0.15406143504021938\n",
            "Validation Loss 0.2527657731674438\n",
            "Validation distance 4.690625\n",
            "Epoch 5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0005]\n",
            "Training Loss 0.15129083010561645\n",
            "Validation Loss 0.25133286485838335\n",
            "Validation distance 4.65906007751938\n",
            "Epoch 6\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0005]\n",
            "Training Loss 0.15093122584445157\n",
            "Validation Loss 0.2534739657543426\n",
            "Validation distance 4.644961240310077\n",
            "Epoch 7\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0005]\n",
            "Training Loss 0.1452085621709973\n",
            "Validation Loss 0.2501321972109551\n",
            "Validation distance 4.592078488372093\n",
            "Epoch 8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0005]\n",
            "Training Loss 0.14323820224144068\n",
            "Validation Loss 0.24973455625911092\n",
            "Validation distance 4.524297480620155\n",
            "Epoch 9\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0005]\n",
            "Training Loss 0.14027587947553669\n",
            "Validation Loss 0.2505492651185324\n",
            "Validation distance 4.539825581395349\n",
            "Epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0005]\n",
            "Training Loss 0.1392762515634335\n",
            "Validation Loss 0.2557891676592272\n",
            "Validation distance 4.558357558139535\n",
            "Epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0005]\n",
            "Training Loss 0.1365125044438012\n",
            "Validation Loss 0.2533000716289809\n",
            "Validation distance 4.574636627906977\n",
            "Epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0005]\n",
            "Training Loss 0.13603902866131412\n",
            "Validation Loss 0.25401984224485796\n",
            "Validation distance 4.503076550387597\n",
            "Epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00025]\n",
            "Training Loss 0.13280361712088884\n",
            "Validation Loss 0.2539481134262196\n",
            "Validation distance 4.4984738372093025\n",
            "Epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:  23%|██▎       | 380/1626 [07:01<22:51,  1.10s/it, loss=0.1219]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-55e7ebe5dddc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_distance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-e4e809cd2763>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"{:.04f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Predictions and Submit to Kaggle"
      ],
      "metadata": {
        "id": "M2H4EEj-sD32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_test = CTCBeamDecoder(LABELS, beam_width=train_config['beam_width'], log_probs_input=True)\n",
        "\n",
        "def make_output(h, lh, decoder, LABELS):\n",
        "  \n",
        "    beam_results, beam_scores, timesteps, out_seq_len = decoder.decode(h, lh) #TODO: What parameters would the decode function take in?\n",
        "    batch_size = h.shape[0]\n",
        "\n",
        "    dist = 0\n",
        "    preds = []\n",
        "    for i in range(batch_size): # Loop through each element in the batch\n",
        "\n",
        "        h_sliced = beam_results[i,0,:out_seq_len[i,0]] #TODO: Obtain the beam results\n",
        "        h_string = ''.join(LABELS[j] for j in h_sliced) #TODO: Convert the beam results to phonemes\n",
        "        preds.append(h_string)\n",
        "    \n",
        "    return preds"
      ],
      "metadata": {
        "id": "2moYJhTWsOG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(test_loader, model, decoder, LABELS):\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(test_loader), dynamic_ncols=True, position=0, leave=False, desc='Test')\n",
        "    predictions = []\n",
        "\n",
        "    for i, data in enumerate(tqdm(test_loader)):\n",
        "        m_pad, m_len = data\n",
        "        m_pad = m_pad.to(device)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            out, out_lengths = model(m_pad, m_len)\n",
        "            preds = make_output(out, out_lengths, decoder, LABELS)\n",
        "\n",
        "        predictions.extend(preds)\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "        del m_pad, m_len\n",
        "        del preds\n",
        "        torch.cuda.empty_cache()\n",
        "  \n",
        "    batch_bar.close()\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "1w9TnDKFLIPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "predictions = predict(test_loader, model, decoder, LABELS)\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/hw3p2/test-clean/transcript/random_submission.csv')\n",
        "df.label = predictions\n",
        "\n",
        "df.to_csv('submission.csv', index = False)\n",
        "# !kaggle competitions submit -c 11-785-f22-hw3p2 -f submission.csv -m \"Message\" \"I made it!\""
      ],
      "metadata": {
        "id": "d70dvu_lsMlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f848156-69d6-4322-8814-f34ff7aa3378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test:   0%|          | 0/41 [00:00<?, ?it/s]\n",
            "Test:   2%|▏         | 1/41 [00:00<00:28,  1.39it/s]\n",
            "Test:   5%|▍         | 2/41 [00:01<00:37,  1.05it/s]\n",
            "Test:   7%|▋         | 3/41 [00:02<00:31,  1.19it/s]\n",
            "Test:  10%|▉         | 4/41 [00:03<00:27,  1.36it/s]\n",
            "Test:  12%|█▏        | 5/41 [00:03<00:27,  1.32it/s]\n",
            "Test:  15%|█▍        | 6/41 [00:04<00:29,  1.18it/s]\n",
            "Test:  17%|█▋        | 7/41 [00:06<00:32,  1.05it/s]\n",
            "Test:  20%|█▉        | 8/41 [00:06<00:29,  1.11it/s]\n",
            "Test:  22%|██▏       | 9/41 [00:07<00:29,  1.08it/s]\n",
            "Test:  24%|██▍       | 10/41 [00:08<00:29,  1.06it/s]\n",
            "Test:  27%|██▋       | 11/41 [00:09<00:27,  1.09it/s]\n",
            "Test:  29%|██▉       | 12/41 [00:10<00:24,  1.19it/s]\n",
            "Test:  32%|███▏      | 13/41 [00:10<00:19,  1.41it/s]\n",
            "Test:  34%|███▍      | 14/41 [00:11<00:21,  1.23it/s]\n",
            "Test:  37%|███▋      | 15/41 [00:12<00:22,  1.15it/s]\n",
            "Test:  39%|███▉      | 16/41 [00:13<00:22,  1.11it/s]\n",
            "Test:  41%|████▏     | 17/41 [00:14<00:19,  1.25it/s]\n",
            "Test:  44%|████▍     | 18/41 [00:15<00:19,  1.17it/s]\n",
            "Test:  46%|████▋     | 19/41 [00:16<00:19,  1.13it/s]\n",
            "Test:  49%|████▉     | 20/41 [00:17<00:19,  1.09it/s]\n",
            "Test:  51%|█████     | 21/41 [00:18<00:19,  1.03it/s]\n",
            "Test:  54%|█████▎    | 22/41 [00:19<00:16,  1.13it/s]\n",
            "Test:  56%|█████▌    | 23/41 [00:19<00:14,  1.28it/s]\n",
            "Test:  59%|█████▊    | 24/41 [00:20<00:15,  1.13it/s]\n",
            "Test:  61%|██████    | 25/41 [00:21<00:13,  1.18it/s]\n",
            "Test:  63%|██████▎   | 26/41 [00:22<00:13,  1.13it/s]\n",
            "Test:  66%|██████▌   | 27/41 [00:23<00:12,  1.11it/s]\n",
            "Test:  68%|██████▊   | 28/41 [00:24<00:11,  1.15it/s]\n",
            "Test:  71%|███████   | 29/41 [00:25<00:11,  1.08it/s]\n",
            "Test:  73%|███████▎  | 30/41 [00:26<00:09,  1.14it/s]\n",
            "Test:  76%|███████▌  | 31/41 [00:26<00:07,  1.25it/s]\n",
            "Test:  78%|███████▊  | 32/41 [00:27<00:07,  1.25it/s]\n",
            "Test:  80%|████████  | 33/41 [00:28<00:07,  1.10it/s]\n",
            "Test:  83%|████████▎ | 34/41 [00:29<00:06,  1.11it/s]\n",
            "Test:  85%|████████▌ | 35/41 [00:30<00:05,  1.15it/s]\n",
            "Test:  88%|████████▊ | 36/41 [00:31<00:04,  1.19it/s]\n",
            "Test:  90%|█████████ | 37/41 [00:32<00:03,  1.05it/s]\n",
            "Test:  93%|█████████▎| 38/41 [00:33<00:02,  1.03it/s]\n",
            "Test:  95%|█████████▌| 39/41 [00:34<00:01,  1.09it/s]\n",
            "Test:  98%|█████████▊| 40/41 [00:34<00:00,  1.18it/s]\n",
            "Test: 100%|██████████| 41/41 [00:35<00:00,  1.22it/s]\n",
            "100%|██████████| 41/41 [00:35<00:00,  1.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "403 - Forbidden\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c 11-785-f22-hw3p2-slack -f submission.csv -m \"Message\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVJj0NFLK9Zt",
        "outputId": "d1e7bbd3-d2ba-4bfd-a2e6-086eab9b19c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.8)\n",
            "100% 209k/209k [00:00<00:00, 1.01MB/s]\n",
            "Successfully submitted to Automatic Speech Recognition (ASR - SLACK)"
          ]
        }
      ]
    }
  ]
}